{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a01137646/ComputerVision/blob/main/4_convolution/4_convolution_juancarlos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6itKzJv8Av"
      },
      "source": [
        "# 4. Image Convolution\n",
        "\n",
        "## Table of Contents\n",
        "1. [Libraries](#libraries)\n",
        "2. [Simple Example](#simple)\n",
        "3. [PyTorch Convolution](#pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mX3Ehc3v8Aw"
      },
      "source": [
        "## Importing Libraries <a class=\"anchor\" id=\"libraries\" ></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hlyqOOHv8Aw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g6f5b9Cv8Ax"
      },
      "source": [
        "## Simple Convolution <a class=\"anchor\" id=\"simple\" ></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh-mZRa4v8Ax"
      },
      "source": [
        "### Definition\n",
        "\n",
        "- __I__: Image to convolve.\n",
        "- __H__: filter matrix to convolve the image with.\n",
        "- __J__: Result of the convolution.\n",
        "\n",
        "The following graphics shows exemplary the mathematical operations of the convolution. The filter matrix __H__ is shifted over the input image __I__. The values 'under' the filter matrix are multiplicated with the corresponding values in __H__, summed up and writen to the result __J__. The target position is usually the position under the center of __H__.\n",
        "\n",
        "<img src=\"data/convolution.png\" width=\"70%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlhP8qBfv8Ax"
      },
      "source": [
        "In order to implement the convolution with a block filter, we need two methods. The first one will create the block filter matrix __H__ depending on the filter width/height __n__.\n",
        "\n",
        "A block filter holds the value $\\dfrac{1}{n\\cdot n}$ at each position:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqsQ2_m-v8Ax"
      },
      "outputs": [],
      "source": [
        "def block_filter(n):\n",
        "    H = np.ones((n, n)) / (n * n) # each element in H has the value 1/(n*n)\n",
        "    return H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPgqve_Fv8Ay"
      },
      "source": [
        "We will test the method by creating a filter with ``n = 5``:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaCzwOPHv8Ay",
        "outputId": "8339a7f7-06da-42a6-8eaf-79924dbb70a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.04 0.04 0.04 0.04 0.04]\n",
            " [0.04 0.04 0.04 0.04 0.04]\n",
            " [0.04 0.04 0.04 0.04 0.04]\n",
            " [0.04 0.04 0.04 0.04 0.04]\n",
            " [0.04 0.04 0.04 0.04 0.04]]\n"
          ]
        }
      ],
      "source": [
        "H = block_filter(5)\n",
        "print(H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrGmAhR5v8Ay"
      },
      "source": [
        "Next, we define the actual convolution operation. To prevent invalid indices at the border of the image, we introduce the padding __p__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8wa05BUv8Ay"
      },
      "outputs": [],
      "source": [
        "def apply_filter(I, H):\n",
        "    h, w = I.shape                         # image dimensions (height, width)\n",
        "    n = H.shape[0]                         # filter size\n",
        "    p = n // 2                             # padding size\n",
        "    J = np.zeros_like(I)                   # output image, initialized with zeros\n",
        "\n",
        "    for x in range(p, h-p):\n",
        "        for y in range(p, w-p):\n",
        "            J[x, y] = np.sum(I[x-p:x+n-p, y-p:y+n-p] * H)\n",
        "    return J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BTDlm8Uv8Az",
        "outputId": "1fc65d9e-a414-4d89-fad2-5fb1ffc34b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/image.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-eaaffca4a9cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/image.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert image to black and white\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/image.jpg'"
          ]
        }
      ],
      "source": [
        "image = Image.open('data/image.jpg')\n",
        "image = image.convert('1') # convert image to black and white\n",
        "\n",
        "image = np.array(image)\n",
        "\n",
        "# image = np.zeros((200, 200), dtype=np.float)\n",
        "# for x in range(200):\n",
        "#     for y in range(200):\n",
        "#         d = ((x-100)**2+(y-100)**2)**0.5\n",
        "#         image[x, y] = d % 8 < 4\n",
        "\n",
        "plt.imshow(image, cmap='gray',vmin=0.0, vmax=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-gin88sv8Az"
      },
      "outputs": [],
      "source": [
        "image = image.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGRruxlyv8Az"
      },
      "source": [
        "Next we test our implementation and apply a block filter with size 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mu04lq0v8Az"
      },
      "outputs": [],
      "source": [
        "n = 7\n",
        "H = block_filter(n)\n",
        "J = apply_filter(image, H)\n",
        "\n",
        "plt.imshow(J, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXFS5aDGv8Az"
      },
      "source": [
        "## PyTorch Convolution <a class=\"anchor\" id=\"pytorch\" ></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHICC3jqv8Az"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open('data/image.jpg')\n",
        "img.thumbnail((256,256), Image.ANTIALIAS) # Resize to half to reduce the size of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alkcX5Hsv8Az"
      },
      "outputs": [],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2achsHYev8Az"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "from torchvision import transforms\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDkSMnt4v8Az"
      },
      "outputs": [],
      "source": [
        "to_tensor = transforms.Compose([\n",
        "   transforms.Grayscale(),  # Convert image to grayscale.\n",
        "   transforms.ToTensor()    # Converts a PIL Image in the range [0, 255] to a torch.FloatTensor in the range [0.0, 1.0].\n",
        "])\n",
        "\n",
        "to_pil = transforms.Compose([\n",
        "    transforms.ToPILImage()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xClQjNp3v8A0"
      },
      "outputs": [],
      "source": [
        "input = to_tensor(img)\n",
        "input.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPdthFbrv8A0"
      },
      "outputs": [],
      "source": [
        "to_pil(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jGAkrDKv8A0"
      },
      "source": [
        "2D convolution over an input image:\n",
        "\n",
        "+ `in_channels = 1`: an input is a grayscale image\n",
        "+ `out_channels = 1`: an output is a grayscale image\n",
        "+ `kernel_size = (3, 3)`: the kernel (filter) size is 3 x 3\n",
        "+ `stride = 1`: the stride for the cross-correlation is 1\n",
        "+ `padding = 1`: zero-paddings on both sides for 1 point for each dimension\n",
        "+ `bias = False`: no bias parameter (for simplicity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xGUkWQnv8A0"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(1, 1, (3, 3), stride=1, padding=1, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4UEgXUwv8A0"
      },
      "outputs": [],
      "source": [
        "# The code below does not work because the convolution layer requires the dimension for batch.\n",
        "conv(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8knybY_v8A0"
      },
      "source": [
        "We need to insert a dimension for a batch at dim=0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ6OO2Ezv8A0"
      },
      "outputs": [],
      "source": [
        "input = input.unsqueeze(0)\n",
        "input.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgKI4UIDv8A0"
      },
      "outputs": [],
      "source": [
        "output = conv(input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slTdw_RAv8A0"
      },
      "source": [
        "Setting `padding=1` in the convolution layer, we obtain an image of the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHOJR5_Wv8A0"
      },
      "outputs": [],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RagsZnVTv8A1"
      },
      "source": [
        "We need to remove the first dimension before converting to a PIL object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7avxnTECv8A1"
      },
      "outputs": [],
      "source": [
        "output.data.squeeze(dim=0).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmltU1Pjv8A1"
      },
      "source": [
        "Display the output from the convolution layer by converting `output` to a PIL object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvcFQagav8A1"
      },
      "outputs": [],
      "source": [
        "to_pil(output.data.squeeze(dim=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id4dGYn6v8A1"
      },
      "source": [
        "Clip every value in the output tensor within the range of [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClE2uzhsv8A1"
      },
      "outputs": [],
      "source": [
        "to_pil(torch.clamp(output, 0, 1).data.squeeze(dim=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz6kyXcKv8A1"
      },
      "outputs": [],
      "source": [
        "def display(img1, img2):\n",
        "    im1 = to_pil(torch.clamp(img1, 0, 1).data.squeeze(dim=0))\n",
        "    im2 = to_pil(torch.clamp(img2, 0, 1).data.squeeze(dim=0))\n",
        "    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n",
        "    dst.paste(im1, (0, 0))\n",
        "    dst.paste(im2, (im1.width, 0))\n",
        "    return dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7f4gBlfv8A1"
      },
      "outputs": [],
      "source": [
        "display(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuoEe87fv8A1"
      },
      "source": [
        "### Identity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "400rTiyAv8A7"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.tensor([[[\n",
        "    [0., 0., 0.],\n",
        "    [0., 1, 0.],\n",
        "    [0., 0., 0.],\n",
        "]]])\n",
        "\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-6MvszYv8A7"
      },
      "source": [
        "### Brighten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wod0U-_Xv8A8"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.tensor([[[\n",
        "    [0., 0., 0.],\n",
        "    [0., 1.5, 0.],\n",
        "    [0., 0., 0.],\n",
        "]]])\n",
        "print(conv.weight.data)\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zsediMxv8A8"
      },
      "source": [
        "### Darken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L72-S2Oav8A8"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.tensor([[[\n",
        "    [0., 0., 0.],\n",
        "    [0., 0.5, 0.],\n",
        "    [0., 0., 0.],\n",
        "]]])\n",
        "print(conv.weight.data)\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkSnEmbvv8A8"
      },
      "source": [
        "### Box blur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D3VNXgrv8A8"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.ones((1, 1, 3,3), dtype=torch.float) / 9.\n",
        "print(conv.weight.data)\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcyOCGpZv8A8"
      },
      "source": [
        "### Gaussian blur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inRz-rc6v8A8"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.tensor([[[\n",
        "    [1., 2., 1.],\n",
        "    [2., 4., 2.],\n",
        "    [1., 2., 1.],\n",
        "]]])/16.\n",
        "print(conv.weight.data)\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6FSgiytv8A8"
      },
      "source": [
        "### Sharpen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFiC780gv8A8"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.tensor([[[\n",
        "    [0., -1., 0.],\n",
        "    [-1., 5., -1.],\n",
        "    [0., -1., 0.],\n",
        "]]])\n",
        "print(conv.weight.data)\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXyT7Jecv8A8"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.tensor([[[\n",
        "    [0., -2., 0.],\n",
        "    [-2., 10., -2.],\n",
        "    [0., -2., 0.],\n",
        "]]])\n",
        "print(conv.weight.data)\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3e2Mz0Fv8A8"
      },
      "source": [
        "### Edge detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6yT8SjUv8A9"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.tensor([[[\n",
        "    [0., 1., 0.],\n",
        "    [1., -4., 1.],\n",
        "    [0., 1., 0.],\n",
        "]]])\n",
        "print(conv.weight.data)\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tvc9aAjv8A9"
      },
      "outputs": [],
      "source": [
        "conv.weight.data = torch.tensor([[[\n",
        "    [-1., -1., -1.],\n",
        "    [-1., 8., -1.],\n",
        "    [-1., -1., -1.],\n",
        "]]])\n",
        "output = conv(input)\n",
        "display(input, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP5PVpnbv8A9"
      },
      "outputs": [],
      "source": [
        "# TODO: Challenge, hard image with hard transformations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PhD",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e17d129d6f27db8b87f58b5fa141e866dbba91479774a0f83bbb21f59b590d58"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}